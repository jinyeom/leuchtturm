{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tf_a3c.png\" align=right width=40%></img>\n",
    "# TensorFlow Implementation of A3C\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)\n",
    "\n",
    "## Contents\n",
    "- [Actor-Critic](#Actor-Critic)\n",
    "- [Worker](#Worker)\n",
    "- [Coffee break](#Coffee-break)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we implement the **Asynchronous Advantage Actor-Critic (A3C)** algorithm with **[TensorFlow](https://www.tensorflow.org/)**. The focus of this project is to (1) learn how to work with TensorFlow in more depth, and (2) have my own working implementation of A3C algorithm for other current or future reinforcement learning projects. For the simplicity of this notebook, we will aim to build an agent to play an Atari 2600 game, **Pong**. \n",
    "\n",
    "Note that **A2C** (synchronous variation of A3C) has been shown to be more cost effective when trained on a single GPU, or TPU, for that matter. We will first try our A3C implementation on a *multi-core CPU*, then A2C on a *TPU (Tensor Processing Unit)* to compare their costs and performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticLSTMPolicy(object):\n",
    "    def __init__(self, num_actions, name):\n",
    "        with tf.variable_scope(name):\n",
    "            self.observation = tf.placeholder(tf.float32, shape=[None, 84, 84, 1], name='observation')\n",
    "    \n",
    "            z1 = tf.layers.conv2d(self.observation, 16, 8, strides=(4, 4), activation=tf.nn.relu)\n",
    "            z2 = tf.layers.conv2d(z1, 32, 4, strides=(2, 2), activation=tf.nn.relu) \n",
    "            z3 = tf.layers.dense(tf.layers.flatten(z2), 256, activation=tf.nn.relu)\n",
    "            z3 = tf.expand_dims(z3, axis=1) # [batch_size, max_time, 256]\n",
    "\n",
    "            lstm_cell = tf.nn.rnn_cell.LSTMCell(256)\n",
    "            initial_state = lstm_cell.zero_state(1, dtype=tf.float32)\n",
    "            z4, self.hidden = tf.nn.dynamic_rnn(lstm_cell, z3, initial_state=initial_state)\n",
    "            z4 = tf.reshape(z4, [-1, 256])\n",
    "\n",
    "            self.actor = tf.layers.dense(z4, num_actions, activation=tf.nn.softmax)\n",
    "            self.critic = tf.layers.dense(z4, 1)\n",
    "            \n",
    "    def __call__(self, sess, observation):\n",
    "        feed_dict = {self.observation: observation}\n",
    "        action_probs, value = sess.run([self.actor, self.critic], feed_dict=feed_dict)\n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly test it before we move on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = [[0.2500289  0.24956998 0.24932864 0.25107247]], v = [[0.0030722]]\n",
      "p = [[0.25029135 0.24919741 0.24967816 0.25083306]], v = [[0.00383127]]\n",
      "p = [[0.2507164  0.24996355 0.24951242 0.24980763]], v = [[-0.0002115]]\n",
      "p = [[0.25096047 0.24900228 0.25008363 0.24995358]], v = [[0.00470447]]\n",
      "p = [[0.25009653 0.24965948 0.24969296 0.25055102]], v = [[-0.00051197]]\n",
      "p = [[0.25060332 0.24939239 0.2492773  0.25072697]], v = [[0.00434569]]\n",
      "p = [[0.25048476 0.24971694 0.24918285 0.2506155 ]], v = [[0.00751681]]\n",
      "p = [[0.2502596  0.25017738 0.24922073 0.25034225]], v = [[0.00192816]]\n",
      "p = [[0.24972083 0.25049695 0.24911007 0.25067207]], v = [[0.00181042]]\n",
      "p = [[0.25010926 0.2501092  0.24939291 0.25038865]], v = [[0.00428144]]\n"
     ]
    }
   ],
   "source": [
    "ac_lstm_policy = ActorCriticLSTMPolicy(4, name='test')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(10):\n",
    "        observation = np.random.normal(loc=0.0, scale=0.1, size=(1, 84, 84, 1))\n",
    "        p, v = ac_lstm_policy(sess, observation)\n",
    "        print(f\"p = {p}, v = {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we're going to build actor networks that will actually interact with their copies of the environment and update their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorPolicy(ActorCriticLSTMPolicy):\n",
    "    def __init__(self, num_actions, name):\n",
    "        super(ActorPolicy, self).__init__(num_actions, name)\n",
    "        with tf.variable_scope(name):\n",
    "            # TODO: add members needed for updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coffee break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    master_network = Agent(a_size, name='global')\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(DoomGame(),i,s_size,a_size,trainer,model_path,global_episodes))\n",
    "    saver = tf.train.Saver(max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
