{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ppo_cover.jpg\" width=25% align=\"right\"/>\n",
    "# Proximal Policy Optimization Algorithms\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)  \n",
    "Original authors: John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n",
    "\n",
    "## Contents\n",
    "- [Implementation](#Implementation)\n",
    "    - [Environment](#Environment)\n",
    "    - [Policy](#Policy)\n",
    "    - [PPO](#PPO)\n",
    "- [Training](#Training)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347)** algorithms are a set of policy gradient algorithms with a novel loss function,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L^{CLIP}(\\theta) = E[min(r_t(\\theta)A_t, clip(r_t(\\theta)A_t, 1 - \\epsilon, 1 + \\epsilon)A_t)] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which extends [TRPO algorithm](https://arxiv.org/abs/1502.05477), but is simpler to implement while showing SOTA performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation refers to [OpenAI's TensorFlow implementation](https://github.com/openai/baselines). In this notebook, however, we're going to be using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NatureDQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, act_dim=18):\n",
    "        super(NatureDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, act_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 1693362\n"
     ]
    }
   ],
   "source": [
    "model = NatureDQN()\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {n_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, t):\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"t = {t}\")\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"SpaceInvaders-v0\")\n",
    "obs = env.reset()\n",
    "for t in range(1000):\n",
    "    if t % 10 == 0:\n",
    "        render(env, t)\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://arxiv.org/abs/1707.06347 (Proximal Policy Optimization Algorithms)\n",
    "- https://arxiv.org/abs/1502.05477 (Trust Region Policy Optimization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
