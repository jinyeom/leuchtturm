{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dqn.jpg\" align=right width=25%></img>\n",
    "# Deep Q-Learning\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)\n",
    "\n",
    "## Contents\n",
    "- [Configuration](#Configuration)\n",
    "- [Environment](#Environment)\n",
    "- [Replay memory](#Replay-memory)\n",
    "- [Deep Q-network](#Deep-Q-network)\n",
    "- [Training loop](#Training-loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is PyTorch implementation of Deep Q-Learning based on [this tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import gym\n",
    "from gym import ObservationWrapper\n",
    "from gym.spaces.box import Box\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from baselines.common import atari_wrappers\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = 42\n",
    "# batch sample size for training\n",
    "BATCH_SIZE = 128\n",
    "# discount factor (gamma)\n",
    "GAMMA = 0.999\n",
    "# start value of epsilon for random actions (epsilon-greedy)\n",
    "EPS_START = 0.9\n",
    "# final value of epsilon for random actions (epsilon-greedy)\n",
    "EPS_END = 0.05\n",
    "# decay rate of epsilon (epsilon-greedy)\n",
    "EPS_DECAY = 200\n",
    "# update rate of target network\n",
    "TARGET_UPDATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes some preprocessing of observations from the Atari environment. This could have been very tedious, but [OpenAI's baseline implementation](https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py) already provides preprocessing code for Atari environments. Thanks, OpenAI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchWrapper(ObservationWrapper):\n",
    "    r\"\"\"ObservationWrapper that outputs observation with the shape of (channels, height, width).\n",
    "    NOTE: adopted from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr.\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(TorchWrapper, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = Box(self.observation_space.low[0, 0, 0],\n",
    "                                     self.observation_space.high[0, 0, 0],\n",
    "                                     [obs_shape[2], obs_shape[1], obs_shape[0]],\n",
    "                                     dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, t):\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"t = {t}\")\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_atari(env_id):\n",
    "    env = atari_wrappers.make_atari(env_id)\n",
    "    env = atari_wrappers.wrap_deepmind(env)\n",
    "    env = TorchWrapper(env)\n",
    "    env.seed(SEED)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for some Jupyter magic! Let's take a look at what the agent sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = make_atari(\"SpaceInvadersNoFrameskip-v4\")\n",
    "obs = env.reset()\n",
    "\n",
    "t = done = 0\n",
    "while not done:\n",
    "    if t % 10 == 0:\n",
    "        render(env, t)\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self._capacity = capacity\n",
    "        self._memory = []\n",
    "        self._position = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._memory)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        if len(self._memory) < self._capacity:\n",
    "            self._memory.append(None) # make room for later\n",
    "        self._memory[self._position] = Transition(*args)\n",
    "        # if the memory is full, start pushing the beginning\n",
    "        self._position = (self._position + 1) % self._capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self._memory, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're going to build the Q-network, based on the original architecture used in [DeepMind's Nature article](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NatureDQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, act_dim=18):\n",
    "        super(NatureDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, act_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 20, 20]           8,224\n",
      "            Conv2d-2             [-1, 64, 9, 9]          32,832\n",
      "            Conv2d-3             [-1, 64, 7, 7]          36,928\n",
      "            Linear-4                  [-1, 512]       1,606,144\n",
      "            Linear-5                   [-1, 18]           9,234\n",
      "================================================================\n",
      "Total params: 1,693,362\n",
      "Trainable params: 1,693,362\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = NatureDQN().to(device)\n",
    "summary(model, (4, 84, 84))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari(\"SpaceInvadersNoFrameskip-v4\")\n",
    "\n",
    "policy_net = NatureDQN()\n",
    "target_net = NatureDQN()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
