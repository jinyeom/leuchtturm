{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ppo_cover.jpg\" width=25% align=\"right\"/>\n",
    "# Proximal Policy Optimization Algorithms\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)  \n",
    "Original authors: John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n",
    "\n",
    "## Contents\n",
    "- [Configuration](#Configuration)\n",
    "- [Environment](#Environment)\n",
    "- [Policy](#Policy)\n",
    "- [PPO](#PPO)\n",
    "- [Training](#Training)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347)** algorithms are a set of policy gradient algorithms with a novel loss function,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L^{CLIP}(\\theta) = E[min(r_t(\\theta)A_t, clip(r_t(\\theta)A_t, 1 - \\epsilon, 1 + \\epsilon)A_t)] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which extends [TRPO algorithm](https://arxiv.org/abs/1502.05477), but is simpler to implement while showing SOTA performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tnrange\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = 42\n",
    "# discount rate\n",
    "GAMMA = 0.99\n",
    "# learning rate\n",
    "ALPHA = 3e-3\n",
    "# number of episodes of training\n",
    "N_EPISODES = 2000\n",
    "# number of iterations per episode\n",
    "N_ITERS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deivce = cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"deivce =\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(SEED)\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode(object):\n",
    "    def __init__(self):\n",
    "        self.observations = [] # agent observation (same as state in MDP)\n",
    "        self.actions = [] # selected action (index)\n",
    "        self.act_probs = [] # probability of each action\n",
    "        self.log_probs = [] # log probability of each action\n",
    "        self.rewards = [] # extrinsic reward signals\n",
    "        \n",
    "    def returns(self, gamma, normalize=True):\n",
    "        returns = [0.0]\n",
    "        for i, r in enumerate(reversed(self.rewards)):\n",
    "            returns.append(r + gamma * returns[i])\n",
    "        returns = torch.tensor(list(reversed(returns[1:])))\n",
    "        if normalize:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        return returns\n",
    "            \n",
    "    def append(self, observation, action, reward):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, 128)\n",
    "        self.fc_actor = nn.Linear(128, act_dim)\n",
    "        self.fc_critic = nn.Linear(128, 1)\n",
    "        \n",
    "    def actor(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc_actor(x), dim=-1)\n",
    "    \n",
    "    def critic(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc_critic(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        act_probs = F.softmax(self.fc_actor(x), dim=-1)\n",
    "        value = self.fc_critic(x)\n",
    "        return act_probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_action(act_probs):\n",
    "    dist = Categorical(act_probs)\n",
    "    action = dist.sample()\n",
    "    return action.item(), dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]             640\n",
      "            Linear-2                    [-1, 2]             258\n",
      "            Linear-3                    [-1, 1]             129\n",
      "================================================================\n",
      "Total params: 1,027\n",
      "Trainable params: 1,027\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n\n",
    "model = ActorCritic(obs_dim, act_dim).to(DEVICE)\n",
    "summary(model, (obs_dim,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(model, epsiode):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, model, update, n_ep, n_iter):\n",
    "    model.train()\n",
    "    for ep in tnrange(n_ep, desc=\"episode\"):\n",
    "        episode = Episode()\n",
    "        obs = env.reset()\n",
    "        for i in range(n_iter):\n",
    "            obs = torch.tensor(obs).float()\n",
    "            act_probs, value = model(obs)\n",
    "            action, act_log_prob = sel_action(act_probs)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            episode.append(obs, action, reward)\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # update the model\n",
    "        update(model, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97127c1cd18346feb118b9d8d69573d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='episode', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(env, model, update, 1, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://arxiv.org/abs/1707.06347 (Proximal Policy Optimization Algorithms)\n",
    "- https://arxiv.org/abs/1502.05477 (Trust Region Policy Optimization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
