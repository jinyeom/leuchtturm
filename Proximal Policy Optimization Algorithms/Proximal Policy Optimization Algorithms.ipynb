{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ppo_cover.jpg\" width=25% align=\"right\"/>\n",
    "# Proximal Policy Optimization Algorithms\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)  \n",
    "Original authors: John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n",
    "\n",
    "## Contents\n",
    "- [Implementation](#Implementation)\n",
    "    - [Environment](#Environment)\n",
    "    - [Policy](#Policy)\n",
    "    - [PPO](#PPO)\n",
    "- [Training](#Training)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347)** algorithms are a set of policy gradient algorithms with a novel loss function,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L^{CLIP}(\\theta) = E[min(r_t(\\theta)A_t, clip(r_t(\\theta)A_t, 1 - \\epsilon, 1 + \\epsilon)A_t)] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which extends [TRPO algorithm](https://arxiv.org/abs/1502.05477), but is simpler to implement while showing SOTA performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation refers to [OpenAI's TensorFlow implementation](https://github.com/openai/baselines), as it provides not only a working implementation of the algorithm, but also because it includes other components that make the algorithm work, e.g., a wrapper for Atari environments that is better to work with. This notebook will focus on learning how the algorithm works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NatureDQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, act_dim=18):\n",
    "        super(NatureDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, act_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 1693362\n"
     ]
    }
   ],
   "source": [
    "model = NatureDQN()\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {n_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, t):\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"t = {t}\")\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = wrap_deepmind(make_atari(\"SpaceInvadersNoFrameskip-v4\"))\n",
    "obs = env.reset()\n",
    "for t in range(1000):\n",
    "    if t % 10 == 0:\n",
    "        render(env, t)\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://arxiv.org/abs/1707.06347 (Proximal Policy Optimization Algorithms)\n",
    "- https://arxiv.org/abs/1502.05477 (Trust Region Policy Optimization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
