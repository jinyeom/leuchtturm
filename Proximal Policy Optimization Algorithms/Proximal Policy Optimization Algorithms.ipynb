{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ppo_cover.jpg\" width=25% align=\"right\"/>\n",
    "# Proximal Policy Optimization Algorithms\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)  \n",
    "Original authors: John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n",
    "\n",
    "## Contents\n",
    "- [Configuration](#Configuration)\n",
    "- [Environment](#Environment)\n",
    "- [Policy](#Policy)\n",
    "- [PPO](#PPO)\n",
    "- [Training](#Training)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347)** algorithms are a set of policy gradient algorithms with a novel loss function,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L^{CLIP}(\\theta) = E[min(r_t(\\theta)A_t, clip(r_t(\\theta)A_t, 1 - \\epsilon, 1 + \\epsilon)A_t)] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which extends [TRPO algorithm](https://arxiv.org/abs/1502.05477), but is simpler to implement while showing SOTA performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "GAMMA = 0.99\n",
    "ALPHA = 3e-3\n",
    "N_EPISODES = 2000\n",
    "N_ITERS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deivce = cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"deivce =\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(SEED)\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, 128)\n",
    "        self.fc_actor = nn.Linear(128, act_dim)\n",
    "        self.fc_critic = nn.Linear(128, 1)\n",
    "        \n",
    "    def actor(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc_actor(x), dim=1)\n",
    "    \n",
    "    def critic(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc_critic(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        act_probs = F.softmax(self.fc_actor(x), dim=1)\n",
    "        value = self.fc_critic(x)\n",
    "        return act_probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_action(act_probs):\n",
    "    r\"\"\"Given a vector of action probabilities, return a selected action\n",
    "    and its log-probability.\n",
    "    \"\"\"\n",
    "    dist = Categorical(act_probs)\n",
    "    action = dist.sample()\n",
    "    return action.item(), dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]             640\n",
      "            Linear-2                    [-1, 2]             258\n",
      "            Linear-3                    [-1, 1]             129\n",
      "================================================================\n",
      "Total params: 1,027\n",
      "Trainable params: 1,027\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n\n",
    "model = ActorCritic(obs_dim, act_dim).to(DEVICE)\n",
    "summary(model, (obs_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode(object):\n",
    "    def __init__(self, gamma):\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def ()\n",
    "    \n",
    "    def append(self, observation, action, reward):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.reward.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ac(model, observations, actions, rewards):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coffee break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, model, update):\n",
    "    for ep in range(N_EPISODES):\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        obs = env.reset()\n",
    "        for i in range(N_ITERS):\n",
    "            act_probs, value = model(obs)\n",
    "            action, act_log_prob = sel_action(act_probs)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "        \n",
    "            # record this episode\n",
    "            # TODO: replace this with a proper Episode container\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # update the model\n",
    "        update(model, observations, actions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://arxiv.org/abs/1707.06347 (Proximal Policy Optimization Algorithms)\n",
    "- https://arxiv.org/abs/1502.05477 (Trust Region Policy Optimization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
