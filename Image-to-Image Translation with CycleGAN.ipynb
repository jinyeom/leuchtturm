{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-to-Image Translation with CycleGAN\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CycleGAN](images/cyclegan.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wget\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(name: str):\n",
    "    if name not in [\"ae_photos\", \"apple2orange\", \"summer2winter_yosemite\", \"horse2zebra\", \n",
    "                    \"monet2photo\", \"cezanne2photo\", \"ukiyoe2photo\", \"vangogh2photo\", \"maps\", \n",
    "                    \"cityscapes\", \"facades\", \"iphone2dslr_flower\", \"ae_photos\"]:\n",
    "        raise ValueError(\"invalid argument dataset name\")\n",
    "        \n",
    "    if not os.path.exists(\"./datasets\"):\n",
    "        print(\"Datasets directory not found, creating a new directory 'datasets'...\")\n",
    "        os.mkdir(\"./datasets\")\n",
    "    zip_path = \"./datasets/{}.zip\".format(name)\n",
    "    target_dir = \"./datasets/{}/\".format(name)\n",
    "    \n",
    "    url = \"https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/{}.zip\".format(name)\n",
    "    wget.download(url, out=zip_path)\n",
    "    \n",
    "    os.mkdir(target_dir)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"datasets/\")\n",
    "    os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets directory not found, creating a new directory 'datasets'...\n"
     ]
    }
   ],
   "source": [
    "# NOTE: only download if you have to!\n",
    "download_dataset(\"apple2orange\")\n",
    "download_dataset(\"horse2zebra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple2orange  horse2zebra\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](images/CycleGAN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.pad = torch.nn.ReflectionPad2d((kernel_size - 1) // 2)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.norm1 = torch.nn.InstanceNorm2d(out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride)\n",
    "        self.norm2 = torch.nn.InstanceNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.pad(x)\n",
    "        z = self.conv1(z)\n",
    "        z = self.norm1(x)\n",
    "        z = self.relu(z)\n",
    "        z = self.pad(z)\n",
    "        z = self.conv2(z)\n",
    "        z = self.norm2(z)\n",
    "        return x + z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we define the architecture of the generator network. Note that the generator network's architecture in CycleGAN is somewhat different from what we're familiar with. Rather than decoding a random Gaussian noise to an image, this generator adopts the mechanism of an image transformation network, which is often used in applications like style transfer and super-resolution. In the origianl paper, 6 residual blocks were used for 128 x 128 images, and 9 for 256 x 256 images. We're going to assume the latter for now, but this can change in the future, based on the desired resolution of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        self.encoder = torch.nn.Sequential(torch.nn.ReflectionPad2d(3),\n",
    "                                           torch.nn.Conv2d(in_channel, 32, 7, 1),\n",
    "                                           torch.nn.InstanceNorm2d(32),\n",
    "                                           torch.nn.ReLU(),\n",
    "                                           torch.nn.Conv2d(32, 64, 3, 2),\n",
    "                                           torch.nn.InstanceNorm2d(64),\n",
    "                                           torch.nn.ReLU(),\n",
    "                                           torch.nn.Conv2d(64, 128, 3, 2),\n",
    "                                           torch.nn.InstanceNorm2d(128),\n",
    "                                           torch.nn.ReLU())\n",
    "        self.transformer = torch.nn.Sequential(ResBlock(128, 128, 3, 1),\n",
    "                                               ResBlock(128, 128, 3, 1),\n",
    "                                               ResBlock(128, 128, 3, 1),\n",
    "                                               ResBlock(128, 128, 3, 1),\n",
    "                                               ResBlock(128, 128, 3, 1),\n",
    "                                               ResBlock(128, 128, 3, 1),\n",
    "                                               ResBlock(128, 128, 3, 1),\n",
    "                                               ResBlock(128, 128, 3, 1),\n",
    "                                               ResBlock(128, 128, 3, 1))\n",
    "        self.decoder = torch.nn.Sequential(torch.nn.ConvTranspose2d(128, 64, 3, 2),\n",
    "                                           torch.nn.ReLU(),\n",
    "                                           torch.nn.InstanceNorm2d(64),\n",
    "                                           torch.nn.ConvTranspose2d(64, 32, 3, 2),\n",
    "                                           torch.nn.ReLU(),\n",
    "                                           torch.nn.ReflectionPad2d(3))\n",
    "        self.output = torch.nn.Sequential(torch.nn.Conv2d(32, 3, 7, 1),\n",
    "                                          torch.nn.InstanceNorm2d(3),\n",
    "                                          torch.nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriminator network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes the descriminator network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def descriminator(input_, name):\n",
    "    with tf.variable_scope(name):\n",
    "        h_0 = tf_conv_block(input_, 64, 7, 2, activation=leaky_relu, normalize=False, name=\"h_0\")\n",
    "        h_1 = tf_conv_block(h_0, 128, 7, 2, activation=leaky_relu, name=\"h_1\")\n",
    "        h_2 = tf_conv_block(h_1, 256, 7, 2, activation=leaky_relu, name=\"h_2\")\n",
    "        h_3 = tf_conv_block(h_2, 512, 7, 2, activation=leaky_relu, name=\"h_3\")\n",
    "        pred = tf_conv_block(h_3, 1, 7, 1, 1, normalize=False, name=\"pred\")\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://arxiv.org/pdf/1703.10593.pdf (Orignal CycleGAN paper)\n",
    "2. https://arxiv.org/pdf/1603.08155v1.pdf (Perceptual losses and image transformation network)\n",
    "3. https://arxiv.org/pdf/1607.08022.pdf (Instance normalization)\n",
    "4. https://hardikbansal.github.io/CycleGANBlog/ (TensorFlow tutorial for CycleGAN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
