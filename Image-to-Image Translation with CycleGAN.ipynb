{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-to-Image Translation with CycleGAN\n",
    "Notebook Author: Jin Yeom (jinyeom@utexas.edu)  \n",
    "Original Authors: Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "import wget\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CycleGAN](images/cyclegan.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_dataset(name: str):\n",
    "    if name not in [\"ae_photos\", \"apple2orange\", \"summer2winter_yosemite\", \"horse2zebra\", \n",
    "                    \"monet2photo\", \"cezanne2photo\", \"ukiyoe2photo\", \"vangogh2photo\", \"maps\", \n",
    "                    \"cityscapes\", \"facades\", \"iphone2dslr_flower\", \"ae_photos\"]:\n",
    "        raise ValueError(\"invalid argument dataset name\")\n",
    "        \n",
    "    if not os.path.exists(\"./datasets\"):\n",
    "        print(\"Datasets directory not found, creating a new directory 'datasets'...\")\n",
    "        os.mkdir(\"./datasets\")\n",
    "    zip_path = \"./datasets/{}.zip\".format(name)\n",
    "    target_dir = \"./datasets/{}/\".format(name)\n",
    "    \n",
    "    url = \"https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/{}.zip\".format(name)\n",
    "    wget.download(url, out=zip_path)\n",
    "    \n",
    "    os.mkdir(target_dir)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"datasets/\")\n",
    "    os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets directory not found, creating a new directory 'datasets'...\n"
     ]
    }
   ],
   "source": [
    "# NOTE: only download if you have to!\n",
    "download_dataset(\"apple2orange\")\n",
    "download_dataset(\"horse2zebra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple2orange  horse2zebra\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testA  testB  trainA  trainB\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./datasets/horse2zebra/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "horse2zebra = datasets.ImageFolder(root=\"./datasets/horse2zebra\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageFolder' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5bf9983cbc59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhorse2zebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageFolder' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "images, labels = horse2zebra.next()\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model](images/CycleGAN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.pad = torch.nn.ReflectionPad2d((kernel_size - 1) // 2)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.norm1 = torch.nn.InstanceNorm2d(out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride)\n",
    "        self.norm2 = torch.nn.InstanceNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.pad(x)\n",
    "        z = self.conv1(z)\n",
    "        z = self.norm1(x)\n",
    "        z = self.relu(z)\n",
    "        z = self.pad(z)\n",
    "        z = self.conv2(z)\n",
    "        z = self.norm2(z)\n",
    "        return x + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channel: int, out_channel: int):\n",
    "        super(Generator, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.model = torch.nn.Sequential(\n",
    "            # encoder\n",
    "            torch.nn.ReflectionPad2d(3), \n",
    "            torch.nn.Conv2d(in_channel, 32, 7, 1),\n",
    "            torch.nn.InstanceNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, 2),\n",
    "            torch.nn.InstanceNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 128, 3, 2),\n",
    "            torch.nn.InstanceNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            # transformer\n",
    "            ResBlock(128, 128, 3, 1),\n",
    "            ResBlock(128, 128, 3, 1),\n",
    "            ResBlock(128, 128, 3, 1),\n",
    "            ResBlock(128, 128, 3, 1),\n",
    "            ResBlock(128, 128, 3, 1),\n",
    "            ResBlock(128, 128, 3, 1),\n",
    "            ResBlock(128, 128, 3, 1),\n",
    "            ResBlock(128, 128, 3, 1),\n",
    "            ResBlock(128, 128, 3, 1),\n",
    "            # decoder\n",
    "            # TODO: switch from deconvolution to resize-convolution upsampling\n",
    "            # to avoid checkerboard artifacts (https://distill.pub/2016/deconv-checkerboard/)\n",
    "            torch.nn.ConvTranspose2d(128, 64, 3, 2), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.InstanceNorm2d(64),\n",
    "            torch.nn.ConvTranspose2d(64, 32, 3, 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ReflectionPad2d(3),\n",
    "            # output\n",
    "            torch.nn.Conv2d(32, 3, 7, 1), \n",
    "            torch.nn.InstanceNorm2d(3),\n",
    "            torch.nn.Tanh())\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Descriminator(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.model = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channel, 64, 7, 2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(64, 128, 7, 2),\n",
    "            torch.nn.InstanceNorm2d(128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(128, 256, 7, 2),\n",
    "            torch.nn.InstanceNorm2d(256),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(256, 512, 7, 2),\n",
    "            torch.nn.InstanceNorm2d(512),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(512, out_channel, 7, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of loss functions are used to train CycleGAN models: **adversarial losses** for matching the distribution of generated images to the data distribution in the target domain (since we're mapping $A$ to $B$ and $B$ to $A$, we need two adversarial loss functions, $G_{AB}: A \\rightarrow B$ and $G_{BA}: B \\rightarrow A$); and **cycle consistency losses** to prevent the learned mappings between the two input domains from contradicting each other. The loss functions are expressed as follows,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}_{GAN}(G_{AB}, D_B, A, B) = \n",
    "    \\mathbb{E}_{b \\sim p_{data(b)}} \\big[ log D_B(b) \\big] +\n",
    "    \\mathbb{E}_{a \\sim p_{data(a)}} \\big[ log(1 - D_B(G(a))) \\big]$$\n",
    "\n",
    "$$\\mathcal{L}_{GAN}(G_{BA}, D_A, B, A) = \n",
    "    \\mathbb{E}_{a \\sim p_{data(a)}} \\big[ log D_A(a) \\big] +\n",
    "    \\mathbb{E}_{b \\sim p_{data(b)}} \\big[ log(1 - D_A(G(b))) \\big]$$\n",
    "\n",
    "$$\\mathcal{L}_{cyc}(G_{XY}, G_{YX}) = \n",
    "    \\mathbb{E}_{x \\sim p_{data(x)}}\n",
    "        \\big[ {\\left\\lVert G_{YX}(G_{XY}(x)) - x \\right\\rVert}_1 \\big] +\n",
    "    \\mathbb{E}_{y \\sim p_{data(y)}}\n",
    "        \\big[ {\\left\\lVert G_{XY}(G_{YX}(y)) - y \\right\\rVert}_1 \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}(G_{AB}, G_{BA}, D_A, D_B) = \n",
    "    \\mathcal{L}_{GAN}(G_{AB}, D_B, A, B)\n",
    "    + \\mathcal{L}_{GAN}(G_{BA}, D_A, B, A)\n",
    "    + \\lambda\\mathcal{L}_{cyc}(G_{XY}, G_{YX})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the adversarial loss is essentially the **binary cross entropy** between outputs of a discriminator network from real and fake data. Also note that Zhu et al. state in their paper that using a **least-squares loss** instead of the negative log likelihood (BCE) improved stability during training and generates higher quality result. We'll try them ourselves and see the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adv_loss_bce = torch.nn.BCELoss() # binary cross entropy (negative log likelihood)\n",
    "adv_loss_mse = torch.nn.MSELoss() # mean squared error (least-squares loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://arxiv.org/pdf/1703.10593.pdf (Orignal CycleGAN paper)\n",
    "2. https://arxiv.org/pdf/1603.08155v1.pdf (Perceptual losses and image transformation network)\n",
    "3. https://arxiv.org/pdf/1607.08022.pdf (Instance normalization)\n",
    "4. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
