{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deep_ga.png\" align=right width=50%></img>\n",
    "# Deep Neuroevolution\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)\n",
    "\n",
    "## Contents\n",
    "- [Configuration](#Configuration)\n",
    "- [Genotype](#Genotype)\n",
    "- [Phenotype](#Phenotype)\n",
    "- [Environment](#Environment)\n",
    "- [Genetic Algorithm (GA)](#Genetic-Algorithm-%28GA%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gym\n",
    "from gym import ObservationWrapper\n",
    "from gym.spaces.box import Box\n",
    "from baselines.common import atari_wrappers\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import functional as T\n",
    "from torchsummary import summary\n",
    "from deap import creator, base, tools\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE =\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation for adding noise each mutation\n",
    "SIGMA = 0.005\n",
    "# environment name\n",
    "ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "# environment seed\n",
    "ENV_SEED = 42\n",
    "# observation screen size\n",
    "SCREEN_SIZE = 84\n",
    "# number of channels (stacked frames)\n",
    "N_CHAN = 4\n",
    "# maximum iterations during evaluation\n",
    "MAX_ITER = 5000\n",
    "# number of generations during evolution\n",
    "N_GEN = 1000\n",
    "# population size\n",
    "POP_SIZE = 1000\n",
    "# number of selected individuals each generation\n",
    "N_SEL = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_seed():\n",
    "    return random.randint(0, 2**31-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(ind):\n",
    "    ind.append(rand_seed())\n",
    "    return deepcopy(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(genotype, model, sigma):\n",
    "    # NOTE: making a copy of the template model only makes sense in parallel settings\n",
    "    # model = deepcopy(model)\n",
    "    init_seed, mut_seeds = genotype[0], genotype[1:]\n",
    "    # initialize the model using initialization seed\n",
    "    torch.manual_seed(init_seed)\n",
    "    def init_weights(m):\n",
    "        if type(m) in {nn.Conv2d, nn.Linear}:\n",
    "            torch.nn.init.xavier_normal_(m.weight)\n",
    "    model.apply(init_weights)\n",
    "    # mutate the model using mutation seeds\n",
    "    for seed in mut_seeds:\n",
    "        torch.manual_seed(seed)\n",
    "        for param in model.parameters():\n",
    "            param.data.add_(torch.randn_like(param) * sigma)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooh, looks like we're going to have to implement the phenotype before testing `decode`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NatureDQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, act_dim=18):\n",
    "        super(NatureDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, act_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return F.softmax(self.fc5(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the network. Remember, `TMPL_MODEL` below will be used to decode each individual genotype during evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 20, 20]           8,224\n",
      "            Conv2d-2             [-1, 64, 9, 9]          32,832\n",
      "            Conv2d-3             [-1, 64, 7, 7]          36,928\n",
      "            Linear-4                  [-1, 512]       1,606,144\n",
      "            Linear-5                   [-1, 18]           9,234\n",
      "================================================================\n",
      "Total params: 1,693,362\n",
      "Trainable params: 1,693,362\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "TMPL_MODEL = NatureDQN().to(DEVICE)\n",
    "summary(TMPL_MODEL, (4, SCREEN_SIZE, SCREEN_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchWrapper(ObservationWrapper):\n",
    "    r\"\"\"ObservationWrapper that outputs observation with the shape of (channels, height, width).\n",
    "    NOTE: adopted from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr.\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(TorchWrapper, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = Box(self.observation_space.low[0, 0, 0],\n",
    "                                     self.observation_space.high[0, 0, 0],\n",
    "                                     [obs_shape[2], obs_shape[1], obs_shape[0]],\n",
    "                                     dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return torch.tensor(observation.transpose(2, 0, 1)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_atari(env_id):\n",
    "    env = atari_wrappers.make_atari(env_id)\n",
    "    env = atari_wrappers.wrap_deepmind(env)\n",
    "    env = TorchWrapper(env)\n",
    "    env.seed(ENV_SEED)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, t):\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"t = {t}\")\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = make_atari(ENV_NAME)\n",
    "obs = env.reset()\n",
    "\n",
    "t = done = 0\n",
    "while not done:\n",
    "    if t % 10 == 0:\n",
    "        render(env, t)\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to what the game renders, below shows what each agent actually sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs[0])\n",
    "plt.title(\"Observation\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ind):\n",
    "    policy = decode(ind, TMPL_MODEL, SIGMA)\n",
    "    policy.eval()\n",
    "    \n",
    "    env = make_atari(ENV_NAME)\n",
    "    stacked_obs = [env.reset() for _ in range(4)]\n",
    "    t = done = fitness = 0\n",
    "    while not done and t < MAX_ITER:\n",
    "        act_probs = policy(torch.cat(stacked_obs, dim=0))\n",
    "        action = torch.argmax(act_probs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        stacked_obs.remove(0) # remove the first frame\n",
    "        stacked_obs.append(obs) # add the next frame\n",
    "        fitness += reward\n",
    "        t += 1\n",
    "    return fitness,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm (GA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, rand_seed, n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"mutate\", mutate)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"select\", tools.selBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"mean\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "elite = tools.HallOfFame(1)\n",
    "logbook = tools.Logbook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea6c617f688423fac3f893c4c6dce05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='gen.', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected stride to be a single integer value or a list of 1 values to match the convolution dimensions, but got stride=[4, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-f993e45ae6ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minvalid_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-e7717798cf52>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(ind)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mMAX_ITER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-fa601e487e84>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected stride to be a single integer value or a list of 1 values to match the convolution dimensions, but got stride=[4, 4]"
     ]
    }
   ],
   "source": [
    "population = toolbox.population(POP_SIZE)\n",
    "for gen in tqdm(range(N_GEN), desc=\"gen.\"):\n",
    "    # evaluate the population\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "        \n",
    "    # update hall of fame and record evaluations\n",
    "    hof.update(population)\n",
    "    record = stats.compile(population)\n",
    "    logbook.record(gen=gen, evals=len(invalid_ind), **record)\n",
    "    \n",
    "    # selection and mutation to update population\n",
    "    elite = deepcopy(hof[0]) # updated elite after evaluation\n",
    "    selected = toolbox.select(population, k=N_SEL)\n",
    "    offsprings = [toolbox.mutate(random.choice(selected)) for _ in range(POP_SIZE - 1)]\n",
    "    population = offsprings + [elite]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
