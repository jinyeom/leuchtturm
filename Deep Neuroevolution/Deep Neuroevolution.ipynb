{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deep_ga.png\" align=right width=50%></img>\n",
    "# Deep Neuroevolution\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)\n",
    "\n",
    "## Contents\n",
    "- [Configuration](#Configuration)\n",
    "- [Genotype](#Genotype)\n",
    "- [Phenotype](#Phenotype)\n",
    "- [Environment](#Environment)\n",
    "- [Genetic Algorithm (GA)](#Genetic-Algorithm-%28GA%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gym\n",
    "from gym import ObservationWrapper\n",
    "from gym.spaces.box import Box\n",
    "from baselines.common import atari_wrappers\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import functional as T\n",
    "from torchsummary import summary\n",
    "from deap import creator, base, tools\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE =\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation for adding noise each mutation\n",
    "SIGMA = 0.005\n",
    "# environment name\n",
    "ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "# environment seed\n",
    "ENV_SEED = 42\n",
    "# observation screen size\n",
    "SCREEN_SIZE = 84\n",
    "# number of channels (stacked frames)\n",
    "N_CHAN = 4\n",
    "# maximum iterations during evaluation\n",
    "MAX_ITER = 5000\n",
    "# number of generations during evolution\n",
    "N_GEN = 1000\n",
    "# population size\n",
    "POP_SIZE = 1000\n",
    "# number of selected individuals each generation\n",
    "N_SEL = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_seed():\n",
    "    return random.randint(0, 2**31-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(ind):\n",
    "    ind.append(rand_seed())\n",
    "    return deepcopy(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(genotype, model, sigma):\n",
    "    # NOTE: making a copy of the template model only makes sense in parallel settings\n",
    "    # model = deepcopy(model)\n",
    "    init_seed, mut_seeds = genotype[0], genotype[1:]\n",
    "    # initialize the model using initialization seed\n",
    "    torch.manual_seed(init_seed)\n",
    "    def init_weights(m):\n",
    "        if type(m) in {nn.Conv2d, nn.Linear}:\n",
    "            torch.nn.init.xavier_normal_(m.weight)\n",
    "    model.apply(init_weights)\n",
    "    # mutate the model using mutation seeds\n",
    "    for seed in mut_seeds:\n",
    "        torch.manual_seed(seed)\n",
    "        for param in model.parameters():\n",
    "            param.data.add_(torch.randn_like(param) * sigma)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooh, looks like we're going to have to implement the phenotype before testing `decode`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NatureDQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, act_dim=18):\n",
    "        super(NatureDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, act_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return F.softmax(self.fc5(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the network. Remember, `TMPL_MODEL` below will be used to decode each individual genotype during evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 20, 20]           8,224\n",
      "            Conv2d-2             [-1, 64, 9, 9]          32,832\n",
      "            Conv2d-3             [-1, 64, 7, 7]          36,928\n",
      "            Linear-4                  [-1, 512]       1,606,144\n",
      "            Linear-5                   [-1, 18]           9,234\n",
      "================================================================\n",
      "Total params: 1,693,362\n",
      "Trainable params: 1,693,362\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "TMPL_MODEL = NatureDQN().to(DEVICE)\n",
    "summary(TMPL_MODEL, (4, SCREEN_SIZE, SCREEN_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchWrapper(ObservationWrapper):\n",
    "    r\"\"\"ObservationWrapper that outputs observation with the shape of (channels, height, width).\n",
    "    NOTE: adopted from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr.\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(TorchWrapper, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = Box(self.observation_space.low[0, 0, 0],\n",
    "                                     self.observation_space.high[0, 0, 0],\n",
    "                                     [obs_shape[2], obs_shape[1], obs_shape[0]],\n",
    "                                     dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_atari(env_id):\n",
    "    env = atari_wrappers.make_atari(env_id)\n",
    "    env = atari_wrappers.wrap_deepmind(env)\n",
    "    env = TorchWrapper(env)\n",
    "    env.seed(ENV_SEED)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, t):\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"t = {t}\")\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = make_atari(ENV_NAME)\n",
    "obs = env.reset()\n",
    "\n",
    "t = done = 0\n",
    "while not done:\n",
    "    if t % 10 == 0:\n",
    "        render(env, t)\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to what the game renders, below shows what each agent actually sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs[0])\n",
    "plt.title(\"Observation\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ind):\n",
    "    policy = decode(ind, TMPL_MODEL, SIGMA)\n",
    "    policy.eval()\n",
    "    \n",
    "    env = make_atari(ENV_NAME)\n",
    "    stacked_obs = [env.reset() for _ in range(4)]\n",
    "    t = done = fitness = 0\n",
    "    while not done and t < MAX_ITER:\n",
    "        act_probs = policy(torch.cat(stacked_obs))\n",
    "        action = torch.argmax(act_probs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        stacked_obs.remove(0) # remove the first frame\n",
    "        stacked_obs.append(obs) # add the next frame\n",
    "        fitness += reward\n",
    "        t += 1\n",
    "    return fitness,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm (GA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initRepeat, list, rand_seed, n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"mutate\", mutate)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"select\", tools.selBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"mean\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "elite = tools.HallOfFame(1)\n",
    "logbook = tools.Logbook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '__dict__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-e376df6939be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPOP_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_GEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gen.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# evaluate the population\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minvalid_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute '__dict__'"
     ]
    }
   ],
   "source": [
    "population = toolbox.population(POP_SIZE)\n",
    "for gen in tqdm(range(N_GEN), desc=\"gen.\"):\n",
    "    # evaluate the population\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "        \n",
    "    # update hall of fame and record evaluations\n",
    "    hof.update(population)\n",
    "    record = stats.compile(population)\n",
    "    logbook.record(gen=gen, evals=len(invalid_ind), **record)\n",
    "    \n",
    "    # selection and mutation to update population\n",
    "    elite = deepcopy(hof[0]) # updated elite after evaluation\n",
    "    selected = toolbox.select(population, k=N_SEL)\n",
    "    offsprings = [toolbox.mutate(random.choice(selected)) for _ in range(POP_SIZE - 1)]\n",
    "    population = offsprings + [elite]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
