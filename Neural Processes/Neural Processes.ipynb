{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Processes\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)\n",
    "\n",
    "## Contents\n",
    "- [Introduction](#Introduction)\n",
    "- [Implementation](#Implementation)\n",
    "- [Experiments](#Experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# NOTE: this notebook requires PyTorch>=1.0 for Cholesky computation\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neural Processes](https://arxiv.org/abs/1807.01622) (NPs) are a novel class of function approximation  methods that was presented by DeepMind, just few months back in July, 2018. Generalizing from their previous work, [Conditional Neural Processes](https://arxiv.org/abs/1807.01613) (CNPs), NPs bring benefits of neural networks and Gaussian processes (GPs), i.e., while being able to estimate the distribution of functions and adapt rapidly to new observations like GPs, they are computationally efficient during training like neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To borrow from DeepMind's [CNP notebook](https://github.com/deepmind/conditional-neural-process/blob/master/conditional_neural_process.ipynb),\n",
    "\n",
    "> [Conditional Neural Processes](https://arxiv.org/pdf/1807.01613.pdf) (CNPs) were introduced as a continuation of [Generative Query Networks](https://deepmind.com/blog/neural-scene-representation-and-rendering/) (GQN) to extend its training regime to tasks beyond scene rendering, e.g. to regression and classification.\n",
    ">\n",
    "> In contrast to most standard neural networks, CNPs learn to approximate a distribution over functions rather than approximating just a single function. As a result, at test time CNPs are flexible and can approximate any function from this distribution when provided with a handful of observations. In addition, they learn to estimate the uncertainty of their prediction from the dataset and as the number of observations is increased this uncertainty reduces and the accuracy of their prediction increases.\n",
    "\n",
    "NPs are simply an extension of CNPs with latent variables that allow global sampling, hence are able to produce different function samples from the same observed context data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NPs consist of three main components:\n",
    "* An **encoder** $h$ which encodes *pairs* of $(x_i, y_i)$ context values to their representations $r_i$ \n",
    "* An **aggregator** $a$ that summarizes encoded context values ($r_1$, $r_2$, ..., $r_n$) to a representation $r$\n",
    "* A **condition decoder** $g$ that takes a sampled $z$ and the new target locations $x_T$ and predicts $y_T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the purpose of this notebook is to explore the basics of NPs, we'll aim to reproduce the 1-D regression result by Garnelo et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, this data generaion code is adopted from DeepMind's CNP notebook, to be used with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The NP takes as input a `CNPRegressionDescription` namedtuple with fields:\n",
    "#   `query`: a tuple containing ((context_x, context_y), target_x)\n",
    "#   `target_y`: a tesor containing the ground truth for the targets to be\n",
    "#     predicted\n",
    "#   `num_total_points`: A vector containing a scalar that describes the total\n",
    "#     number of datapoints used (context + target)\n",
    "#   `num_context_points`: A vector containing a scalar that describes the number\n",
    "#     of datapoints used as context\n",
    "# The GPCurvesReader returns the newly sampled data in this format at each\n",
    "# iteration\n",
    "NPRegressionDescription = collections.namedtuple(\n",
    "        \"NPRegressionDescription\",\n",
    "        (\"query\", \"target_y\", \"num_total_points\", \"num_context_points\"))\n",
    "\n",
    "\n",
    "class GPCurvesReader(object):\n",
    "    \"\"\"Generates curves using a Gaussian Process (GP).\n",
    "\n",
    "    Supports vector inputs (x) and vector outputs (y). Kernel is\n",
    "    mean-squared exponential, using the x-value l2 coordinate distance scaled by\n",
    "    some factor chosen randomly in a range. Outputs are independent gaussian\n",
    "    processes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, max_num_context, x_size=1, y_size=1,\n",
    "            l1_scale=0.4, sigma_scale=1.0, testing=False):\n",
    "        \"\"\"Creates a regression dataset of functions sampled from a GP.\n",
    "\n",
    "        Args:\n",
    "            batch_size: An integer.\n",
    "            max_num_context: The max number of observations in the context.\n",
    "            x_size: Integer >= 1 for length of \"x values\" vector.\n",
    "            y_size: Integer >= 1 for length of \"y values\" vector.\n",
    "            l1_scale: Float; typical scale for kernel distance function.\n",
    "            sigma_scale: Float; typical scale for variance.\n",
    "            testing: Boolean that indicates whether we are testing. If so there are\n",
    "                    more targets for visualization.\n",
    "        \"\"\"\n",
    "        self._batch_size = batch_size\n",
    "        self._max_num_context = max_num_context\n",
    "        self._x_size = x_size\n",
    "        self._y_size = y_size\n",
    "        self._l1_scale = l1_scale\n",
    "        self._sigma_scale = sigma_scale\n",
    "        self._testing = testing\n",
    "\n",
    "    def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
    "        \"\"\"Applies the Gaussian kernel to generate curve data.\n",
    "\n",
    "        Args:\n",
    "            xdata: Tensor with shape `[batch_size, num_total_points, x_size]` with\n",
    "                    the values of the x-axis data.\n",
    "            l1: Tensor with shape `[batch_size, y_size, x_size]`, the scale\n",
    "                    parameter of the Gaussian kernel.\n",
    "            sigma_f: Float tensor with shape `[batch_size, y_size]`; the magnitude\n",
    "                    of the std.\n",
    "            sigma_noise: Float, std of the noise that we add for stability.\n",
    "\n",
    "        Returns:\n",
    "            The kernel, a float tensor with shape\n",
    "            `[batch_size, y_size, num_total_points, num_total_points]`.\n",
    "        \"\"\"\n",
    "        num_total_points = xdata.shape[1]\n",
    "\n",
    "        # Expand and take the difference\n",
    "        xdata1 = xdata.unsqueeze(1)  # [B, 1, num_total_points, x_size]\n",
    "        xdata2 = xdata.unsqueeze(2)  # [B, num_total_points, 1, x_size]\n",
    "        diff = xdata1 - xdata2  # [B, num_total_poinst, num_total_points, x_size]\n",
    "\n",
    "        # [B, y_size, num_total_points, num_total_points, x_size]\n",
    "        norm = (diff[:, None, :, :, :] / l1[:, :, None, None, :])**2\n",
    "        norm = torch.sum(norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
    "\n",
    "        # [B, y_size, num_total_points, num_total_points]\n",
    "        kernel = (sigma_f**2)[:, :, None, None] * torch.exp(-0.5 * norm)\n",
    "\n",
    "        # Add some noise to the diagonal to make the cholesky work.\n",
    "        kernel += (sigma_noise**2) * torch.eye(num_total_points)\n",
    "\n",
    "        return kernel\n",
    "\n",
    "    def generate_curves(self):\n",
    "        \"\"\"Builds the op delivering the data.\n",
    "\n",
    "        Generated functions are `float32` with x values between -2 and 2.\n",
    "    \n",
    "        Returns:\n",
    "            A `NPRegressionDescription` namedtuple.\n",
    "        \"\"\"\n",
    "        num_context = np.random.randint(3, self._max_num_context)\n",
    "\n",
    "        # If we are testing we want to have more targets and have them evenly\n",
    "        # distributed in order to plot the function.\n",
    "        if self._testing:\n",
    "            num_target = 400\n",
    "            num_total_points = num_target\n",
    "            x_values = torch.arange(-2.0, 2.0, 0.01).unsqueeze(0).repeat(self._batch_size, 1)\n",
    "            x_values = x_values.unsqueeze(-1)\n",
    "            \n",
    "        # During training the number of target points and their x-positions are\n",
    "        # selected at random\n",
    "        else:\n",
    "            num_target = np.random.randint(2, self._max_num_context)\n",
    "            num_total_points = num_context + num_target\n",
    "            x_values = 4.0 * torch.rand(self._batch_size, num_total_points, self._x_size) - 2.0\n",
    "\n",
    "        # Set kernel parameters\n",
    "        l1 = torch.ones(self._batch_size, self._y_size, self._x_size) * self._l1_scale\n",
    "        sigma_f = torch.ones(self._batch_size, self._y_size) * self._sigma_scale\n",
    "\n",
    "        # Pass the x_values through the Gaussian kernel\n",
    "        # [batch_size, y_size, num_total_points, num_total_points]\n",
    "        kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
    "        \n",
    "        # Calculate Cholesky, using double precision for better stability:\n",
    "        cholesky = torch.cholesky(kernel.double()).float()\n",
    "\n",
    "        # Sample a curve\n",
    "        # [batch_size, y_size, num_total_points, 1]\n",
    "        y_values = torch.matmul(cholesky, torch.randn(self._batch_size, self._y_size, num_total_points, 1))\n",
    "\n",
    "        # [batch_size, num_total_points, y_size]\n",
    "        y_values = y_values.squeeze(3).permute(0, 2, 1)\n",
    "\n",
    "        if self._testing:\n",
    "            # Select the targets\n",
    "            target_x = x_values\n",
    "            target_y = y_values\n",
    "\n",
    "            # Select the observations\n",
    "            idx = torch.randperm(num_target)\n",
    "            context_x = torch.gather(x_values, 1, idx[:num_context])\n",
    "            context_x = torch.gather(y_values, 1, idx[:num_context])\n",
    "\n",
    "        else:\n",
    "            # Select the targets which will consist of the context points as well as\n",
    "            # some new target points\n",
    "            target_x = x_values[:, :num_target + num_context, :]\n",
    "            target_y = y_values[:, :num_target + num_context, :]\n",
    "\n",
    "            # Select the observations\n",
    "            context_x = x_values[:, :num_context, :]\n",
    "            context_y = y_values[:, :num_context, :]\n",
    "\n",
    "        query = ((context_x, context_y), target_x)\n",
    "\n",
    "        return NPRegressionDescription(\n",
    "                query=query, \n",
    "                target_y=target_y,\n",
    "                num_total_points=target_x.shape[1], \n",
    "                num_context_points=num_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 14, 14])\n",
      "torch.Size([1, 1, 400, 400])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 4: Index tensor must have same dimensions as input tensor at /opt/conda/conda-bld/pytorch-nightly-cpu_1543046796899/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:434",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3075f7b98069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPCurvesReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-e385d00041ee>\u001b[0m in \u001b[0;36mgenerate_curves\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# Select the observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mcontext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_context\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mcontext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_context\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 4: Index tensor must have same dimensions as input tensor at /opt/conda/conda-bld/pytorch-nightly-cpu_1543046796899/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:434"
     ]
    }
   ],
   "source": [
    "# Train dataset\n",
    "dataset_train = GPCurvesReader(batch_size=64, max_num_context=10)\n",
    "data_train = dataset_train.generate_curves()\n",
    "\n",
    "# Test dataset\n",
    "dataset_test = GPCurvesReader(batch_size=1, max_num_context=10, testing=True)\n",
    "data_test = dataset_test.generate_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, cx_size, cy_size, repr_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(cx_size + cy_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, repr_size)\n",
    "        \n",
    "    def forward(self, context_x, context_y):\n",
    "        input_ = torch.cat((context_x, context_y), dim=-1)\n",
    "        input_ = F.relu(self.fc1(input_), inplace=True)\n",
    "        input_ = F.relu(self.fc2(input_), inplace=True)\n",
    "        input_ = F.relu(self.fc3(input_), inplace=True)\n",
    "        repr_ = self.fc4(input_) # representation\n",
    "        # TODO: return the aggregated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDecoder(nn.Module):\n",
    "    def __init__(self, tx_size, repr_size):\n",
    "        super(ConditionalDecoder, self).__init__()\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
