{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Processes\n",
    "Author: Jin Yeom (jinyeom@utexas.edu)\n",
    "\n",
    "## Contents\n",
    "- [Introduction](#Introduction)\n",
    "- [Implementation](#Implementation)\n",
    "- [Experiments](#Experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neural Processes](https://arxiv.org/abs/1807.01622) (NPs) are a novel class of function approximation  methods that was presented by DeepMind, just few months back in July, 2018. Generalizing from their previous work, [Conditional Neural Processes](https://arxiv.org/abs/1807.01613) (CNPs), NPs bring benefits of neural networks and Gaussian processes (GPs), i.e., while being able to estimate the distribution of functions and adapt rapidly to new observations like GPs, they are computationally efficient during training like neural networks.\n",
    "\n",
    "I'm personally fascinated by this new approach, since it addresses two major disadvantages of common deep learning practices I have been very unhappy about: 1) it is able to estimate uncertainty in its prediction, and 2) it can be trained further with additional data without disrupting previously trained knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NPs are consist of three main components:\n",
    "* An **encoder** $h$ which encodes *pairs* of $(x_i, y_i)$ context values to their representations $r_i$ \n",
    "* An **aggregator** $a$ that summarizes encoded context values to a representation $r$, which parameterizes the latent distribution $z \\sim \\mathcal{N}(\\mu(r), I\\sigma(r))$\n",
    "* A **condition decoder** $g$ that takes $z$ and the new target locations $x_T$ and predicts $y_T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, cx_size, cy_size, repr_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(cx_size + cy_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, repr_size)\n",
    "        \n",
    "    def forward(self, context_x, context_y):\n",
    "        input_ = torch.cat((context_x, context_y), dim=-1)\n",
    "        input_ = F.relu(self.fc1(input_), inplace=True)\n",
    "        input_ = F.relu(self.fc2(input_), inplace=True)\n",
    "        input_ = F.relu(self.fc3(input_), inplace=True)\n",
    "        repr_ = self.fc4(input_) # representation\n",
    "        # TODO: return the aggregated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDecoder(nn.Module):\n",
    "    def __init__(self, tx_size, repr_size):\n",
    "        super(ConditionalDecoder, self).__init__()\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
